{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\r\n",
    "\r\n",
    "import fiftyone as fo\r\n",
    "\r\n",
    "import torchvision\r\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\r\n",
    "\r\n",
    "import torch\r\n",
    "import fiftyone.utils.coco as fouc\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader as DataLoader\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\apole\\fiftyone\\coco-2017\\validation'\n",
      "Downloading annotations to 'C:\\Users\\apole\\fiftyone\\coco-2017\\tmp-download\\annotations_trainval2017.zip'\n",
      " 100% |██████|    1.9Gb/1.9Gb [1.6m elapsed, 0s remaining, 20.2Mb/s]      \n",
      "Extracting annotations to 'C:\\Users\\apole\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Downloading images to 'C:\\Users\\apole\\fiftyone\\coco-2017\\tmp-download\\val2017.zip'\n",
      " 100% |██████|    6.1Gb/6.1Gb [8.4m elapsed, 0s remaining, 13.9Mb/s]      \n",
      "Extracting images to 'C:\\Users\\apole\\fiftyone\\coco-2017\\validation\\data'\n",
      "Writing annotations to 'C:\\Users\\apole\\fiftyone\\coco-2017\\validation\\labels.json'\n",
      "Dataset info written to 'C:\\Users\\apole\\fiftyone\\coco-2017\\info.json'\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [1.1m elapsed, 0s remaining, 96.5 samples/s]       \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading your custom data\r\n",
    "\r\n",
    "If you have data that follows a certain format on disk (for example a directory tree for classification, the COCO detection format, or many more), then you can load it into FiftyOne in one line of code:\r\n",
    "\r\n",
    "\r\n",
    "dataset = fo.Dataset(\r\n",
    "    \"/path/to/dataset_dir\", \r\n",
    "    dataset_type=fo.types.COCODetectionDataset, \r\n",
    "    name=\"my_dataset\"\r\n",
    ")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\r\n",
    "    # load a model pre-trained on COCO\r\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n",
    "    \r\n",
    "    # get number of input features for the classifier\r\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n",
    "    \r\n",
    "    # replace the pre-trained head with a new one\r\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\r\n",
    "    \r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\r\n",
    "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        fiftyone_dataset: a FiftyOne dataset or view that will be used for \r\n",
    "            training or testing\r\n",
    "        transforms (None): a list of PyTorch transforms to apply to images \r\n",
    "            and targets when loading\r\n",
    "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset \r\n",
    "            that contains the desired labels to load\r\n",
    "        classes (None): a list of class strings that are used to define the \r\n",
    "            mapping between class names and indices. If None, it will use \r\n",
    "            all classes present in the given fiftyone_dataset.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        fiftyone_dataset,\r\n",
    "        transforms=None,\r\n",
    "        gt_field=\"ground_truth\",\r\n",
    "        classes=None,\r\n",
    "    ):\r\n",
    "        self.samples = fiftyone_dataset\r\n",
    "        self.transforms = transforms\r\n",
    "        self.gt_field = gt_field\r\n",
    "\r\n",
    "        self.img_paths = self.samples.values(\"filepath\")\r\n",
    "\r\n",
    "        self.classes = classes\r\n",
    "        if not self.classes:\r\n",
    "            # Get list of distinct labels that exist in the view\r\n",
    "            self.classes = self.samples.distinct(\r\n",
    "                \"%s.detections.label\" % gt_field\r\n",
    "            )\r\n",
    "\r\n",
    "        if self.classes[0] != \"background\":\r\n",
    "            self.classes = [\"background\"] + self.classes\r\n",
    "\r\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img_path = self.img_paths[idx]\r\n",
    "        sample = self.samples[img_path]\r\n",
    "        metadata = sample.metadata\r\n",
    "        img = Image.open(img_path).convert(\"RGB\")\r\n",
    "\r\n",
    "        boxes = []\r\n",
    "        labels = []\r\n",
    "        area = []\r\n",
    "        iscrowd = []\r\n",
    "        detections = sample[self.gt_field].detections\r\n",
    "        for det in detections:\r\n",
    "            coco_obj = fouc.COCOObject.from_detection(\r\n",
    "                det, metadata, labels_map_rev=self.labels_map_rev\r\n",
    "            )\r\n",
    "            x, y, w, h = coco_obj.bbox\r\n",
    "            boxes.append([x, y, x + w, y + h])\r\n",
    "            labels.append(coco_obj.category_id)\r\n",
    "            area.append(coco_obj.area)\r\n",
    "            iscrowd.append(coco_obj.iscrowd)\r\n",
    "\r\n",
    "        target = {}\r\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\r\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\r\n",
    "        target[\"image_id\"] = torch.as_tensor([idx])\r\n",
    "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\r\n",
    "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\r\n",
    "\r\n",
    "        if self.transforms is not None:\r\n",
    "            img, target = self.transforms(img, target)\r\n",
    "\r\n",
    "        return img, target\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.img_paths)\r\n",
    "\r\n",
    "    def get_classes(self):\r\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FiftyOneTorchDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loader = DataLoader(dataset, batch_size=5, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in my_loader:\r\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}