{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import torch\r\n",
    "import torch.utils.data\r\n",
    "import torchvision\r\n",
    "from PIL import Image\r\n",
    "from pycocotools.coco import COCO\r\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myOwnDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, root, annotation, transforms=None):\r\n",
    "        self.root = root\r\n",
    "        self.transforms = transforms\r\n",
    "        self.coco = COCO(annotation)\r\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        # Own coco file\r\n",
    "        coco = self.coco\r\n",
    "        # Image ID\r\n",
    "        img_id = self.ids[index]\r\n",
    "        # List: get annotation id from coco\r\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\r\n",
    "        # Dictionary: target coco_annotation file for an image\r\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\r\n",
    "        # path for input image\r\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\r\n",
    "        # open the input image\r\n",
    "        img = Image.open(os.path.join(self.root, path))\r\n",
    "\r\n",
    "        # number of objects in the image\r\n",
    "        num_objs = len(coco_annotation)\r\n",
    "\r\n",
    "        # Bounding boxes for objects\r\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\r\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\r\n",
    "        boxes = []\r\n",
    "        for i in range(num_objs):\r\n",
    "            xmin = coco_annotation[i]['bbox'][0]\r\n",
    "            ymin = coco_annotation[i]['bbox'][1]\r\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\r\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\r\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\r\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\r\n",
    "        # Labels (In my case, I only one class: target class or background)\r\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\r\n",
    "        # Tensorise img_id\r\n",
    "        img_id = torch.tensor([img_id])\r\n",
    "        # Size of bbox (Rectangular)\r\n",
    "        areas = []\r\n",
    "        for i in range(num_objs):\r\n",
    "            areas.append(coco_annotation[i]['area'])\r\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\r\n",
    "        # Iscrowd\r\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n",
    "\r\n",
    "        # Annotation is in dictionary format\r\n",
    "        my_annotation = {}\r\n",
    "        my_annotation[\"boxes\"] = boxes\r\n",
    "        my_annotation[\"labels\"] = labels\r\n",
    "        my_annotation[\"image_id\"] = img_id\r\n",
    "        my_annotation[\"area\"] = areas\r\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\r\n",
    "\r\n",
    "        if self.transforms is not None:\r\n",
    "            img = self.transforms(img)\r\n",
    "\r\n",
    "        return img, my_annotation\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my case, just added ToTensor\r\n",
    "def get_transform():\r\n",
    "    custom_transforms = []\r\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\r\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# path to your own data and coco file\r\n",
    "train_data_dir = 'C:/Users/apole/Desktop/AML/ObjectDetection/V1/datasets/coco/images/val2017'\r\n",
    "train_coco = 'C:/Users/apole/Desktop/AML/ObjectDetection/V1/datasets/coco/annotations/captions_val2017.json'\r\n",
    "\r\n",
    "# create own Dataset\r\n",
    "my_dataset = myOwnDataset(root=train_data_dir,\r\n",
    "                          annotation=train_coco,\r\n",
    "                          transforms=get_transform()\r\n",
    "                          )\r\n",
    "\r\n",
    "# collate_fn needs for batch\r\n",
    "def collate_fn(batch):\r\n",
    "    return tuple(zip(*batch))\r\n",
    "\r\n",
    "# Batch size\r\n",
    "train_batch_size = 1\r\n",
    "\r\n",
    "# own DataLoader\r\n",
    "data_loader = torch.utils.data.DataLoader(my_dataset,\r\n",
    "                                          batch_size=train_batch_size,\r\n",
    "                                          shuffle=True,\r\n",
    "                                          num_workers=4,\r\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x215457d2d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data_loader))\r\n",
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device (whether GPU or CPU)\r\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
    "\r\n",
    "# DataLoader is iterable over Dataset\r\n",
    "for imgs, annotations in data_loader:\r\n",
    "    # imgs = list(img.to(device) for img in imgs)\r\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\r\n",
    "    # print(annotations)\r\n",
    "    print(annotations)\r\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}